{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq8fDvrpse4b",
        "outputId": "229ce5f5-bda3-494a-8659-64857b846d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index-core==0.10.30 #The core llamaindex package\n",
        "!pip install -q llama-index-llms-gemini==0.1.7 #The package for Google Gemini LLM\n",
        "!pip install -q llama-index-embeddings-gemini==0.1.6 #The package for using Gemini embeddings\n",
        "!pip install -q llama-index-readers-file==0.1.19 #For reading files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "GOOGLE_API_KEY = getpass('Enter Gemini Key : ')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnojDdH5sp6M",
        "outputId": "8c8a19b5-b131-4020-cf3c-752824317052"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Gemini Key : ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O ./data/llama_2.pdf #Downloading Llama 2 paper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knCytifpszZP",
        "outputId": "3dd8f0a1-f562-4d44-95bc-a48576c5d24a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-03 15:24:51--  https://arxiv.org/pdf/2307.09288.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.131.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2307.09288 [following]\n",
            "--2024-06-03 15:24:51--  http://arxiv.org/pdf/2307.09288\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13661300 (13M) [application/pdf]\n",
            "Saving to: ‘./data/llama_2.pdf’\n",
            "\n",
            "./data/llama_2.pdf  100%[===================>]  13.03M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-06-03 15:24:51 (116 MB/s) - ‘./data/llama_2.pdf’ saved [13661300/13661300]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.settings import Settings\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "Settings.llm = Gemini(model_name=\"models/gemini-pro\", temperature = 0.0)\n",
        "Settings.embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "7FDjPdy1s0mI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader('./data', filename_as_id = True).load_data()"
      ],
      "metadata": {
        "id": "QLstJEy2s24u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD02gn4DtBU9",
        "outputId": "52231605-8a7a-4b8d-d84f-9811c313fa8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we have 1 document the documents is 77 in length. The load data implicitly does node chunking. This behavior can be altered."
      ],
      "metadata": {
        "id": "UVAP8rLHtIAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_documents = SimpleDirectoryReader('./data')"
      ],
      "metadata": {
        "id": "FjsQEyBitG48"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(raw_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9PUzQ8jtbxJ",
        "outputId": "29d97945-559d-4969-f83e-e9040a6f9224"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on SimpleDirectoryReader in module llama_index.core.readers.file.base object:\n",
            "\n",
            "class SimpleDirectoryReader(llama_index.core.readers.base.BaseReader)\n",
            " |  SimpleDirectoryReader(input_dir: Optional[str] = None, input_files: Optional[List] = None, exclude: Optional[List] = None, exclude_hidden: bool = True, errors: str = 'ignore', recursive: bool = False, encoding: str = 'utf-8', filename_as_id: bool = False, required_exts: Optional[List[str]] = None, file_extractor: Optional[Dict[str, llama_index.core.readers.base.BaseReader]] = None, num_files_limit: Optional[int] = None, file_metadata: Optional[Callable[[str], Dict]] = None, raise_on_error: bool = False, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> None\n",
            " |  \n",
            " |  Simple directory reader.\n",
            " |  \n",
            " |  Load files from file directory.\n",
            " |  Automatically select the best file reader given file extensions.\n",
            " |  \n",
            " |  Args:\n",
            " |      input_dir (str): Path to the directory.\n",
            " |      input_files (List): List of file paths to read\n",
            " |          (Optional; overrides input_dir, exclude)\n",
            " |      exclude (List): glob of python file paths to exclude (Optional)\n",
            " |      exclude_hidden (bool): Whether to exclude hidden files (dotfiles).\n",
            " |      encoding (str): Encoding of the files.\n",
            " |          Default is utf-8.\n",
            " |      errors (str): how encoding and decoding errors are to be handled,\n",
            " |            see https://docs.python.org/3/library/functions.html#open\n",
            " |      recursive (bool): Whether to recursively search in subdirectories.\n",
            " |          False by default.\n",
            " |      filename_as_id (bool): Whether to use the filename as the document id.\n",
            " |          False by default.\n",
            " |      required_exts (Optional[List[str]]): List of required extensions.\n",
            " |          Default is None.\n",
            " |      file_extractor (Optional[Dict[str, BaseReader]]): A mapping of file\n",
            " |          extension to a BaseReader class that specifies how to convert that file\n",
            " |          to text. If not specified, use default from DEFAULT_FILE_READER_CLS.\n",
            " |      num_files_limit (Optional[int]): Maximum number of files to read.\n",
            " |          Default is None.\n",
            " |      file_metadata (Optional[Callable[str, Dict]]): A function that takes\n",
            " |          in a filename and returns a Dict of metadata for the Document.\n",
            " |          Default is None.\n",
            " |      raise_on_error (bool): Whether to raise an error if a file cannot be read.\n",
            " |      fs (Optional[fsspec.AbstractFileSystem]): File system to use. Defaults\n",
            " |      to using the local file system. Can be changed to use any remote file system\n",
            " |      exposed via the fsspec interface.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      SimpleDirectoryReader\n",
            " |      llama_index.core.readers.base.BaseReader\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, input_dir: Optional[str] = None, input_files: Optional[List] = None, exclude: Optional[List] = None, exclude_hidden: bool = True, errors: str = 'ignore', recursive: bool = False, encoding: str = 'utf-8', filename_as_id: bool = False, required_exts: Optional[List[str]] = None, file_extractor: Optional[Dict[str, llama_index.core.readers.base.BaseReader]] = None, num_files_limit: Optional[int] = None, file_metadata: Optional[Callable[[str], Dict]] = None, raise_on_error: bool = False, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> None\n",
            " |      Initialize with parameters.\n",
            " |  \n",
            " |  async aload_data(self, show_progress: bool = False, num_workers: Optional[int] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> List[llama_index.core.schema.Document]\n",
            " |      Load data from the input directory.\n",
            " |      \n",
            " |      Args:\n",
            " |          show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n",
            " |          num_workers  (Optional[int]): Number of workers to parallelize data-loading over.\n",
            " |          fs (Optional[fsspec.AbstractFileSystem]): File system to use. If fs was specified\n",
            " |              in the constructor, it will override the fs parameter here.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List[Document]: A list of documents.\n",
            " |  \n",
            " |  async aload_file(self, input_file: pathlib.Path) -> List[llama_index.core.schema.Document]\n",
            " |      Load file asynchronously.\n",
            " |  \n",
            " |  is_hidden(self, path: pathlib.Path) -> bool\n",
            " |  \n",
            " |  iter_data(self, show_progress: bool = False) -> Generator[List[llama_index.core.schema.Document], Any, Any]\n",
            " |      Load data iteratively from the input directory.\n",
            " |      \n",
            " |      Args:\n",
            " |          show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Generator[List[Document]]: A list of documents.\n",
            " |  \n",
            " |  load_data(self, show_progress: bool = False, num_workers: Optional[int] = None, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> List[llama_index.core.schema.Document]\n",
            " |      Load data from the input directory.\n",
            " |      \n",
            " |      Args:\n",
            " |          show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n",
            " |          num_workers  (Optional[int]): Number of workers to parallelize data-loading over.\n",
            " |          fs (Optional[fsspec.AbstractFileSystem]): File system to use. If fs was specified\n",
            " |              in the constructor, it will override the fs parameter here.\n",
            " |      \n",
            " |      Returns:\n",
            " |          List[Document]: A list of documents.\n",
            " |  \n",
            " |  supported_suffix_fn = _try_loading_included_file_formats() -> Dict[str, Type[llama_index.core.readers.base.BaseReader]]\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  load_file(input_file: pathlib.Path, file_metadata: Callable[[str], Dict], file_extractor: Dict[str, llama_index.core.readers.base.BaseReader], filename_as_id: bool = False, encoding: str = 'utf-8', errors: str = 'ignore', raise_on_error: bool = False, fs: Optional[fsspec.spec.AbstractFileSystem] = None) -> List[llama_index.core.schema.Document]\n",
            " |      Static method for loading file.\n",
            " |      \n",
            " |      NOTE: necessarily as a static method for parallel processing.\n",
            " |      \n",
            " |      Args:\n",
            " |          input_file (Path): _description_\n",
            " |          file_metadata (Callable[[str], Dict]): _description_\n",
            " |          file_extractor (Dict[str, BaseReader]): _description_\n",
            " |          filename_as_id (bool, optional): _description_. Defaults to False.\n",
            " |          encoding (str, optional): _description_. Defaults to \"utf-8\".\n",
            " |          errors (str, optional): _description_. Defaults to \"ignore\".\n",
            " |          fs (Optional[fsspec.AbstractFileSystem], optional): _description_. Defaults to None.\n",
            " |      \n",
            " |      input_file (Path): File path to read\n",
            " |      file_metadata ([Callable[str, Dict]]): A function that takes\n",
            " |          in a filename and returns a Dict of metadata for the Document.\n",
            " |      file_extractor (Dict[str, BaseReader]): A mapping of file\n",
            " |          extension to a BaseReader class that specifies how to convert that file\n",
            " |          to text.\n",
            " |      filename_as_id (bool): Whether to use the filename as the document id.\n",
            " |      encoding (str): Encoding of the files.\n",
            " |          Default is utf-8.\n",
            " |      errors (str): how encoding and decoding errors are to be handled,\n",
            " |            see https://docs.python.org/3/library/functions.html#open\n",
            " |      raise_on_error (bool): Whether to raise an error if a file cannot be read.\n",
            " |      fs (Optional[fsspec.AbstractFileSystem]): File system to use. Defaults\n",
            " |          to using the local file system. Can be changed to use any remote file system\n",
            " |      \n",
            " |      Returns:\n",
            " |          List[Document]: loaded documents\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {'supported_suffix_fn': typing.Callable}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from llama_index.core.readers.base.BaseReader:\n",
            " |  \n",
            " |  async alazy_load_data(self, *args: Any, **load_kwargs: Any) -> Iterable[llama_index.core.schema.Document]\n",
            " |      Load data from the input directory lazily.\n",
            " |  \n",
            " |  lazy_load_data(self, *args: Any, **load_kwargs: Any) -> Iterable[llama_index.core.schema.Document]\n",
            " |      Load data from the input directory lazily.\n",
            " |  \n",
            " |  load_langchain_documents(self, **load_kwargs: Any) -> List[ForwardRef('LCDocument')]\n",
            " |      Load data in LangChain document format.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from llama_index.core.readers.base.BaseReader:\n",
            " |  \n",
            " |  __get_pydantic_json_schema__(core_schema, handler) from abc.ABCMeta\n",
            " |  \n",
            " |  __modify_schema__(field_schema: Dict[str, Any], field: Optional[Any]) from abc.ABCMeta\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from llama_index.core.readers.base.BaseReader:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].id_, documents[0].doc_id, documents[0].node_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ParHx92KvNCd",
        "outputId": "1be0333f-57d8-4970-a166-919feaf50ee2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('7cb89ea7-8c50-404d-9964-e3874d5b67a3',\n",
              " '7cb89ea7-8c50-404d-9964-e3874d5b67a3',\n",
              " '7cb89ea7-8c50-404d-9964-e3874d5b67a3')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[1].id_, documents[1].doc_id, documents[1].node_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BF8NupsvYqm",
        "outputId": "148c5d61-a17f-4f36-921d-79d3516c9b46"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('b9dbc756-075d-4859-99b0-4ba78f5d34c2',\n",
              " 'b9dbc756-075d-4859-99b0-4ba78f5d34c2',\n",
              " 'b9dbc756-075d-4859-99b0-4ba78f5d34c2')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting metdata for one of the nodes\n",
        "documents[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC-9UASktfHn",
        "outputId": "48e244f8-6fdf-4e35-a9a3-7f4fe3fe9f77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'page_label': '1',\n",
              " 'file_name': 'llama_2.pdf',\n",
              " 'file_path': '/content/data/llama_2.pdf',\n",
              " 'file_type': 'application/pdf',\n",
              " 'file_size': 13661300,\n",
              " 'creation_date': '2024-06-03',\n",
              " 'last_modified_date': '2023-07-22'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "J2l9TT7Vt9cQ",
        "outputId": "fe721770-6436-4dd8-845a-a8b6c56e2c64"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Llama 2 : Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗Louis Martin†Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom∗\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements of Llama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n†Second author\\nContributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL_YkWWtt_9b",
        "outputId": "5f99e03c-374c-441d-e71b-20986f7b4e8e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on Document in module llama_index.core.schema object:\n",
            "\n",
            "class Document(TextNode)\n",
            " |  Document(*, doc_id: str = None, embedding: Optional[List[float]] = None, extra_info: Dict[str, Any] = None, excluded_embed_metadata_keys: List[str] = None, excluded_llm_metadata_keys: List[str] = None, relationships: Dict[llama_index.core.schema.NodeRelationship, Union[llama_index.core.schema.RelatedNodeInfo, List[llama_index.core.schema.RelatedNodeInfo]]] = None, text: str = '', start_char_idx: Optional[int] = None, end_char_idx: Optional[int] = None, text_template: str = '{metadata_str}\\n\\n{content}', metadata_template: str = '{key}: {value}', metadata_seperator: str = '\\n') -> None\n",
            " |  \n",
            " |  Generic interface for a data document.\n",
            " |  \n",
            " |  This document connects to data sources.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Document\n",
            " |      TextNode\n",
            " |      BaseNode\n",
            " |      BaseComponent\n",
            " |      pydantic.v1.main.BaseModel\n",
            " |      pydantic.v1.utils.Representation\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __setattr__(self, name: str, value: object) -> None\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __str__(self) -> str\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  get_doc_id(self) -> str\n",
            " |      TODO: Deprecated: Get document ID.\n",
            " |  \n",
            " |  to_embedchain_format(self) -> Dict[str, Any]\n",
            " |      Convert struct to EmbedChain document format.\n",
            " |  \n",
            " |  to_haystack_format(self) -> 'HaystackDocument'\n",
            " |      Convert struct to Haystack document format.\n",
            " |  \n",
            " |  to_langchain_format(self) -> 'LCDocument'\n",
            " |      Convert struct to LangChain document format.\n",
            " |  \n",
            " |  to_semantic_kernel_format(self) -> 'MemoryRecord'\n",
            " |      Convert struct to Semantic Kernel document format.\n",
            " |  \n",
            " |  to_vectorflow(self, client: Any) -> None\n",
            " |      Send a document to vectorflow, since they don't have a document object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  class_name() -> str from pydantic.v1.main.ModelMetaclass\n",
            " |      Get the class name, used as a unique ID in serialization.\n",
            " |      \n",
            " |      This provides a key that makes serialization robust against actual class\n",
            " |      name changes.\n",
            " |  \n",
            " |  example() -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  from_embedchain_format(doc: Dict[str, Any]) -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |      Convert struct from EmbedChain document format.\n",
            " |  \n",
            " |  from_haystack_format(doc: 'HaystackDocument') -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |      Convert struct from Haystack document format.\n",
            " |  \n",
            " |  from_langchain_format(doc: 'LCDocument') -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |      Convert struct from LangChain document format.\n",
            " |  \n",
            " |  from_semantic_kernel_format(doc: 'MemoryRecord') -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |      Convert struct from Semantic Kernel document format.\n",
            " |  \n",
            " |  get_type() -> str from pydantic.v1.main.ModelMetaclass\n",
            " |      Get Document type.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  doc_id\n",
            " |      Get document ID.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {'id_': <class 'str'>}\n",
            " |  \n",
            " |  __class_vars__ = set()\n",
            " |  \n",
            " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
            " |  \n",
            " |  __custom_root_type__ = False\n",
            " |  \n",
            " |  __exclude_fields__ = None\n",
            " |  \n",
            " |  __fields__ = {'embedding': ModelField(name='embedding', type=Optional[...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __include_fields__ = None\n",
            " |  \n",
            " |  __post_root_validators__ = []\n",
            " |  \n",
            " |  __pre_root_validators__ = []\n",
            " |  \n",
            " |  __private_attributes__ = {}\n",
            " |  \n",
            " |  __schema_cache__ = {}\n",
            " |  \n",
            " |  __signature__ = <Signature (*, doc_id: str = None, embedding: Op...val...\n",
            " |  \n",
            " |  __validators__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from TextNode:\n",
            " |  \n",
            " |  get_content(self, metadata_mode: llama_index.core.schema.MetadataMode = <MetadataMode.NONE: 'none'>) -> str\n",
            " |      Get object content.\n",
            " |  \n",
            " |  get_metadata_str(self, mode: llama_index.core.schema.MetadataMode = <MetadataMode.ALL: 'all'>) -> str\n",
            " |      Metadata info string.\n",
            " |  \n",
            " |  get_node_info(self) -> Dict[str, Any]\n",
            " |      Get node info.\n",
            " |  \n",
            " |  get_text(self) -> str\n",
            " |  \n",
            " |  set_content(self, value: str) -> None\n",
            " |      Set the content of the node.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from TextNode:\n",
            " |  \n",
            " |  hash\n",
            " |      Get hash of node.\n",
            " |  \n",
            " |  node_info\n",
            " |      Deprecated: Get node info.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseNode:\n",
            " |  \n",
            " |  as_related_node_info(self) -> llama_index.core.schema.RelatedNodeInfo\n",
            " |      Get node as RelatedNodeInfo.\n",
            " |  \n",
            " |  get_embedding(self) -> List[float]\n",
            " |      Get embedding.\n",
            " |      \n",
            " |      Errors if embedding is None.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from BaseNode:\n",
            " |  \n",
            " |  child_nodes\n",
            " |      Child nodes.\n",
            " |  \n",
            " |  extra_info\n",
            " |      TODO: DEPRECATED: Extra info.\n",
            " |  \n",
            " |  next_node\n",
            " |      Next node.\n",
            " |  \n",
            " |  parent_node\n",
            " |      Parent node.\n",
            " |  \n",
            " |  prev_node\n",
            " |      Prev node.\n",
            " |  \n",
            " |  ref_doc_id\n",
            " |      Deprecated: Get ref doc id.\n",
            " |  \n",
            " |  source_node\n",
            " |      Source object node.\n",
            " |      \n",
            " |      Extracted from the relationships field.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseNode:\n",
            " |  \n",
            " |  node_id\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from BaseNode:\n",
            " |  \n",
            " |  Config = <class 'llama_index.core.schema.BaseNode.Config'>\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseComponent:\n",
            " |  \n",
            " |  __getstate__(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  __setstate__(self, state: Dict[str, Any]) -> None\n",
            " |  \n",
            " |  dict(self, **kwargs: Any) -> Dict[str, Any]\n",
            " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
            " |  \n",
            " |  json(self, **kwargs: Any) -> str\n",
            " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
            " |      \n",
            " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
            " |  \n",
            " |  to_dict(self, **kwargs: Any) -> Dict[str, Any]\n",
            " |  \n",
            " |  to_json(self, **kwargs: Any) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from BaseComponent:\n",
            " |  \n",
            " |  from_dict(data: Dict[str, Any], **kwargs: Any) -> typing_extensions.Self from pydantic.v1.main.ModelMetaclass\n",
            " |      # TODO: return type here not supported by current mypy version\n",
            " |  \n",
            " |  from_json(data_str: str, **kwargs: Any) -> typing_extensions.Self from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __init__(__pydantic_self__, **data: Any) -> None\n",
            " |      Create a new model by parsing and validating input data from keyword arguments.\n",
            " |      \n",
            " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
            " |  \n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      so `dict(model)` works\n",
            " |  \n",
            " |  __repr_args__(self) -> 'ReprArgs'\n",
            " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
            " |      \n",
            " |      Can either return:\n",
            " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
            " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
            " |  \n",
            " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
            " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
            " |      \n",
            " |      :param include: fields to include in new model\n",
            " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
            " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
            " |          the new model: you should trust this data\n",
            " |      :param deep: set to `True` to make a deep copy of the model\n",
            " |      :return: new model instance\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
            " |      Same as update_forward_refs but will not raise exception\n",
            " |      when forward references are not defined.\n",
            " |  \n",
            " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
            " |  \n",
            " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
            " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
            " |  \n",
            " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __fields_set__\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.v1.utils.Representation:\n",
            " |  \n",
            " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __repr_name__(self) -> str\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |  \n",
            " |  __repr_str__(self, join_str: str) -> str\n",
            " |  \n",
            " |  __rich_repr__(self) -> 'RichReprResult'\n",
            " |      Get fields for Rich library\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Relationship properties like parent, children, next node, previous node and etc.\n",
        "print('Parent node:', documents[0].parent_node)\n",
        "print('Next node:', documents[0].next_node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBfrps64uB-v",
        "outputId": "ae3689b1-3930-4c43-9d2c-4994c2510267"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parent node: None\n",
            "Next node: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let us see what data will be seen for the embedding node\n",
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "print(documents[0].get_content(metadata_mode = MetadataMode.EMBED))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q45laA6Uus2X",
        "outputId": "17028d97-e975-4061-95e0-c5b17215a14c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_label: 1\n",
            "file_path: /content/data/llama_2.pdf\n",
            "\n",
            "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
            "Hugo Touvron∗Louis Martin†Kevin Stone†\n",
            "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
            "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
            "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
            "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
            "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
            "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
            "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
            "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
            "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
            "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
            "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
            "Sergey Edunov Thomas Scialom∗\n",
            "GenAI, Meta\n",
            "Abstract\n",
            "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
            "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
            "Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
            "models outperform open-source chat models on most benchmarks we tested, and based on\n",
            "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n",
            "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
            "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
            "contribute to the responsible development of LLMs.\n",
            "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
            "†Second author\n",
            "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let us see what data will be seen for the LLM synthesis\n",
        "print(documents[0].get_content(metadata_mode = MetadataMode.LLM))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5YSLzE8wDxD",
        "outputId": "627310d0-9056-471f-96e2-1bf22c1d75db"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_label: 1\n",
            "file_path: /content/data/llama_2.pdf\n",
            "\n",
            "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
            "Hugo Touvron∗Louis Martin†Kevin Stone†\n",
            "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
            "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
            "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
            "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
            "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
            "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
            "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
            "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
            "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
            "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
            "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
            "Sergey Edunov Thomas Scialom∗\n",
            "GenAI, Meta\n",
            "Abstract\n",
            "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
            "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
            "Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
            "models outperform open-source chat models on most benchmarks we tested, and based on\n",
            "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n",
            "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
            "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
            "contribute to the responsible development of LLMs.\n",
            "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
            "†Second author\n",
            "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].excluded_llm_metadata_keys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAP7E6qvwTuJ",
        "outputId": "68528555-e79a-4bc3-9c31-a92ec753b7dd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['file_name',\n",
              " 'file_type',\n",
              " 'file_size',\n",
              " 'creation_date',\n",
              " 'last_modified_date',\n",
              " 'last_accessed_date']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].excluded_llm_metadata_keys.append('file_path')"
      ],
      "metadata": {
        "id": "SF84EhaLweVC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents[0].get_content(metadata_mode = MetadataMode.LLM))\n",
        "#We do not see the file path property here"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1CB63ufw-ue",
        "outputId": "52af54d8-936e-4d38-a533-cda14e4a194b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_label: 1\n",
            "\n",
            "Llama 2 : Open Foundation and Fine-Tuned Chat Models\n",
            "Hugo Touvron∗Louis Martin†Kevin Stone†\n",
            "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
            "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
            "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
            "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
            "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
            "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
            "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
            "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
            "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
            "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
            "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
            "Sergey Edunov Thomas Scialom∗\n",
            "GenAI, Meta\n",
            "Abstract\n",
            "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
            "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
            "Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n",
            "models outperform open-source chat models on most benchmarks we tested, and based on\n",
            "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n",
            "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
            "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
            "contribute to the responsible development of LLMs.\n",
            "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
            "†Second author\n",
            "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata_seperator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DYgrAkp3xCC4",
        "outputId": "0ebd07b2-f661-4e98-a970-1ed9c81ed2db"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZbtWNvjgxKx7",
        "outputId": "fa03dc61-9b19-4573-86dc-10739dee067a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{key}: {value}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].text_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QfaGbDIwxNNU",
        "outputId": "f296c25e-bafe-4df6-af06-decf902d9775"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{metadata_str}\\n\\n{content}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3mEJh6HxQfH",
        "outputId": "a5c15e78-75bd-4d7f-8e75-5b7b66c38a7b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on Document in module llama_index.core.schema object:\n",
            "\n",
            "class Document(TextNode)\n",
            " |  Document(*, doc_id: str = None, embedding: Optional[List[float]] = None, extra_info: Dict[str, Any] = None, excluded_embed_metadata_keys: List[str] = None, excluded_llm_metadata_keys: List[str] = None, relationships: Dict[llama_index.core.schema.NodeRelationship, Union[llama_index.core.schema.RelatedNodeInfo, List[llama_index.core.schema.RelatedNodeInfo]]] = None, text: str = '', start_char_idx: Optional[int] = None, end_char_idx: Optional[int] = None, text_template: str = '{metadata_str}\\n\\n{content}', metadata_template: str = '{key}: {value}', metadata_seperator: str = '\\n') -> None\n",
            " |  \n",
            " |  Generic interface for a data document.\n",
            " |  \n",
            " |  This document connects to data sources.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Document\n",
            " |      TextNode\n",
            " |      BaseNode\n",
            " |      BaseComponent\n",
            " |      pydantic.v1.main.BaseModel\n",
            " |      pydantic.v1.utils.Representation\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __setattr__(self, name: str, value: object) -> None\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __str__(self) -> str\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  get_doc_id(self) -> str\n",
            " |      TODO: Deprecated: Get document ID.\n",
            " |  \n",
            " |  to_embedchain_format(self) -> Dict[str, Any]\n",
            " |      Convert struct to EmbedChain document format.\n",
            " |  \n",
            " |  to_haystack_format(self) -> 'HaystackDocument'\n",
            " |      Convert struct to Haystack document format.\n",
            " |  \n",
            " |  to_langchain_format(self) -> 'LCDocument'\n",
            " |      Convert struct to LangChain document format.\n",
            " |  \n",
            " |  to_semantic_kernel_format(self) -> 'MemoryRecord'\n",
            " |      Convert struct to Semantic Kernel document format.\n",
            " |  \n",
            " |  to_vectorflow(self, client: Any) -> None\n",
            " |      Send a document to vectorflow, since they don't have a document object.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  class_name() -> str from pydantic.v1.main.ModelMetaclass\n",
            " |      Get the class name, used as a unique ID in serialization.\n",
            " |      \n",
            " |      This provides a key that makes serialization robust against actual class\n",
            " |      name changes.\n",
            " |  \n",
            " |  example() -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  from_embedchain_format(doc: Dict[str, Any]) -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |      Convert struct from EmbedChain document format.\n",
            " |  \n",
            " |  from_haystack_format(doc: 'HaystackDocument') -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |      Convert struct from Haystack document format.\n",
            " |  \n",
            " |  from_langchain_format(doc: 'LCDocument') -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |      Convert struct from LangChain document format.\n",
            " |  \n",
            " |  from_semantic_kernel_format(doc: 'MemoryRecord') -> 'Document' from pydantic.v1.main.ModelMetaclass\n",
            " |      Convert struct from Semantic Kernel document format.\n",
            " |  \n",
            " |  get_type() -> str from pydantic.v1.main.ModelMetaclass\n",
            " |      Get Document type.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  doc_id\n",
            " |      Get document ID.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {'id_': <class 'str'>}\n",
            " |  \n",
            " |  __class_vars__ = set()\n",
            " |  \n",
            " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
            " |  \n",
            " |  __custom_root_type__ = False\n",
            " |  \n",
            " |  __exclude_fields__ = None\n",
            " |  \n",
            " |  __fields__ = {'embedding': ModelField(name='embedding', type=Optional[...\n",
            " |  \n",
            " |  __hash__ = None\n",
            " |  \n",
            " |  __include_fields__ = None\n",
            " |  \n",
            " |  __post_root_validators__ = []\n",
            " |  \n",
            " |  __pre_root_validators__ = []\n",
            " |  \n",
            " |  __private_attributes__ = {}\n",
            " |  \n",
            " |  __schema_cache__ = {}\n",
            " |  \n",
            " |  __signature__ = <Signature (*, doc_id: str = None, embedding: Op...val...\n",
            " |  \n",
            " |  __validators__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from TextNode:\n",
            " |  \n",
            " |  get_content(self, metadata_mode: llama_index.core.schema.MetadataMode = <MetadataMode.NONE: 'none'>) -> str\n",
            " |      Get object content.\n",
            " |  \n",
            " |  get_metadata_str(self, mode: llama_index.core.schema.MetadataMode = <MetadataMode.ALL: 'all'>) -> str\n",
            " |      Metadata info string.\n",
            " |  \n",
            " |  get_node_info(self) -> Dict[str, Any]\n",
            " |      Get node info.\n",
            " |  \n",
            " |  get_text(self) -> str\n",
            " |  \n",
            " |  set_content(self, value: str) -> None\n",
            " |      Set the content of the node.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from TextNode:\n",
            " |  \n",
            " |  hash\n",
            " |      Get hash of node.\n",
            " |  \n",
            " |  node_info\n",
            " |      Deprecated: Get node info.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseNode:\n",
            " |  \n",
            " |  as_related_node_info(self) -> llama_index.core.schema.RelatedNodeInfo\n",
            " |      Get node as RelatedNodeInfo.\n",
            " |  \n",
            " |  get_embedding(self) -> List[float]\n",
            " |      Get embedding.\n",
            " |      \n",
            " |      Errors if embedding is None.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from BaseNode:\n",
            " |  \n",
            " |  child_nodes\n",
            " |      Child nodes.\n",
            " |  \n",
            " |  extra_info\n",
            " |      TODO: DEPRECATED: Extra info.\n",
            " |  \n",
            " |  next_node\n",
            " |      Next node.\n",
            " |  \n",
            " |  parent_node\n",
            " |      Parent node.\n",
            " |  \n",
            " |  prev_node\n",
            " |      Prev node.\n",
            " |  \n",
            " |  ref_doc_id\n",
            " |      Deprecated: Get ref doc id.\n",
            " |  \n",
            " |  source_node\n",
            " |      Source object node.\n",
            " |      \n",
            " |      Extracted from the relationships field.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseNode:\n",
            " |  \n",
            " |  node_id\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from BaseNode:\n",
            " |  \n",
            " |  Config = <class 'llama_index.core.schema.BaseNode.Config'>\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseComponent:\n",
            " |  \n",
            " |  __getstate__(self) -> Dict[str, Any]\n",
            " |  \n",
            " |  __setstate__(self, state: Dict[str, Any]) -> None\n",
            " |  \n",
            " |  dict(self, **kwargs: Any) -> Dict[str, Any]\n",
            " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
            " |  \n",
            " |  json(self, **kwargs: Any) -> str\n",
            " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
            " |      \n",
            " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
            " |  \n",
            " |  to_dict(self, **kwargs: Any) -> Dict[str, Any]\n",
            " |  \n",
            " |  to_json(self, **kwargs: Any) -> str\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from BaseComponent:\n",
            " |  \n",
            " |  from_dict(data: Dict[str, Any], **kwargs: Any) -> typing_extensions.Self from pydantic.v1.main.ModelMetaclass\n",
            " |      # TODO: return type here not supported by current mypy version\n",
            " |  \n",
            " |  from_json(data_str: str, **kwargs: Any) -> typing_extensions.Self from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __eq__(self, other: Any) -> bool\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __init__(__pydantic_self__, **data: Any) -> None\n",
            " |      Create a new model by parsing and validating input data from keyword arguments.\n",
            " |      \n",
            " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
            " |  \n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      so `dict(model)` works\n",
            " |  \n",
            " |  __repr_args__(self) -> 'ReprArgs'\n",
            " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
            " |      \n",
            " |      Can either return:\n",
            " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
            " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
            " |  \n",
            " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
            " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
            " |      \n",
            " |      :param include: fields to include in new model\n",
            " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
            " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
            " |          the new model: you should trust this data\n",
            " |      :param deep: set to `True` to make a deep copy of the model\n",
            " |      :return: new model instance\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
            " |      Same as update_forward_refs but will not raise exception\n",
            " |      when forward references are not defined.\n",
            " |  \n",
            " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
            " |  \n",
            " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
            " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
            " |  \n",
            " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __fields_set__\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.v1.utils.Representation:\n",
            " |  \n",
            " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __repr_name__(self) -> str\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |  \n",
            " |  __repr_str__(self, join_str: str) -> str\n",
            " |  \n",
            " |  __rich_repr__(self) -> 'RichReprResult'\n",
            " |      Get fields for Rich library\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aI4vbiHGxSbv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}